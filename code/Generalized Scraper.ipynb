{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find list of items (checklist) we want scraped in the same way (include URLS and IDs (IDs can just be URL)). Finding this checklist is outside of the scope of this file/scraper. \n",
    "2. For each item X we want to scrape:\n",
    "    * Run scraping function on X (and store scrape of X in folder)\n",
    "        * ```scrape_and_save()```\n",
    "    * Update checklist to reflect that X has been scraped (and save updated version of checklist)\n",
    "        * ```update_checklist_table()```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel location of checklist of URLs to scrape\n",
    "CHECKLIST_REL_PATH = '../checklist/checklist.csv'\n",
    "\n",
    "# rel location of folder where we store the scraped data\n",
    "SCRAPED_DATA_FOLDER_REL_PATH = '../scraped_data/'\n",
    "\n",
    "# names of relevant columns in our checklist dataframe\n",
    "CHECKLIST_COL_NAMES=dict(\n",
    "    ID='ID',\n",
    "    URL='Link',\n",
    "    SCRAPED_BOOL='has_been_scraped'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING UNIQUE TO THIS PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "import time\n",
    "\n",
    "def get_a_markets_transactions(market_ID:str, \n",
    "                               delay_mean=.5,\n",
    "                               delay_std=.2) -> pd.DataFrame:\n",
    "    \"\"\"delay is measured in seconds\"\"\"\n",
    "\n",
    "    market_url = 'https://polymarketwhales.info/transactions?orderBy=timestamp&market=' + market_ID\n",
    "    \n",
    "    transactions = []\n",
    "    page_count = 1\n",
    "    still_more_pages = True\n",
    "    \n",
    "    while still_more_pages:\n",
    "        delay = np.random.normal(loc=delay_mean, scale=delay_std, size=1)[0]\n",
    "        time.sleep(delay)\n",
    "        \n",
    "        url_suffix = f'&page={page_count}'\n",
    "        \n",
    "        # transactions_to_append = get_yes_prices_from_url(market_url + url_suffix)\n",
    "        transactions_to_append = pd.read_html(market_url + url_suffix)[0]\n",
    "        \n",
    "        if len(transactions_to_append) > 0:    \n",
    "            transactions.append(transactions_to_append)\n",
    "            page_count += 1\n",
    "        else:\n",
    "            still_more_pages=False       \n",
    "\n",
    "    # return transactions\n",
    "    if page_count > 1:\n",
    "        return pd.concat(transactions)\n",
    "    else:\n",
    "        return transactions[0]\n",
    "\n",
    "def clean_a_markets_transactions(table:pd.DataFrame) -> pd.DataFrame:\n",
    "    \n",
    "    # map inefficient datatypes to ints\n",
    "    table['Type'] = table['Type'].map({'Buy':1, 'Sell':0})\n",
    "    table['Outcome'] = table['Outcome'].map({'Yes':1, 'No':0})\n",
    "    table['Direction'] = table['Direction'].map({'✅':1, '❌':0})\n",
    "\n",
    "    # clean 'Amount' (str-->float)\n",
    "    table['Amount'] = table['Amount'].str.split('$').str[1].astype(float)\n",
    "\n",
    "    # convert to Yes Price\n",
    "    table['Yes Price'] = np.where(table['Outcome']==1, \n",
    "                                  table['Price'],\n",
    "                                  1-table['Price'])\n",
    "        \n",
    "    # reverse order of table to go from first trades to last\n",
    "    table = table[::-1].reset_index()\n",
    "\n",
    "    return table.drop(['index', 'Timestamp', 'Unnamed: 8', 'Unnamed: 9'], axis='columns')\n",
    "\n",
    "def scrape_and_save(URL_to_scrape:str,\n",
    "                    scraped_data_save_path:str,\n",
    "                    scraped_filename:str,\n",
    "                    ID:str) -> int: \n",
    "    \"\"\"\n",
    "    This function grabs data from the URL.\n",
    "    MUST RETURN `True` if the scrape didn't work (i.e., if we want to log np.nan under \"scraped_bool\" field in 'checklist')\n",
    "    \"\"\"\n",
    "\n",
    "    # scrape\n",
    "    try:\n",
    "        df = get_a_markets_transactions(market_ID=ID)\n",
    "        df = clean_a_markets_transactions(df)\n",
    "        df.to_csv(f'{scraped_data_save_path}{scraped_filename}.csv',\n",
    "                  index=False)\n",
    "        something_went_wrong = 0\n",
    "    except:\n",
    "        print('something went wrong with scraping')\n",
    "        something_went_wrong = 1\n",
    "        df = []\n",
    "\n",
    "    # figure out what value to log in the checklist (i.e., was this successfully scraped?)\n",
    "    conditions = (len(df) > 0) and (not something_went_wrong)\n",
    "    if conditions:\n",
    "        bool_to_log = 1.0\n",
    "    else:\n",
    "        bool_to_log = np.nan\n",
    "    return bool_to_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE CHECKLIST TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_checklist_table(checklist_path:str, \n",
    "                        #    checklist:pd.DataFrame,\n",
    "                           scraped_bool_col_:str,\n",
    "                           URL_col:str,\n",
    "                           URL_of_newly_scraped:str,\n",
    "                           bool_to_log_=False\n",
    "                           ):\n",
    "    \n",
    "    global unexpected_chars\n",
    "\n",
    "    # This slows the code down a bit but unless we're scraping 000s of pages this shouldn't be the biggest bottleneck.\n",
    "    # Reading and saving it every time is also more robust to error.\n",
    "    checklist = pd.read_csv(checklist_path) \n",
    "\n",
    "    # confirm that bools are numerical and not True, False\n",
    "    unique_bools = checklist[scraped_bool_col_].dropna().unique().tolist()\n",
    "    if not len(set(unique_bools) - set([0, 1, 0.0, 1.0, np.nan])) == 0: # only contains 0, 1, or np.nan\n",
    "        unexpected_chars = unique_bools\n",
    "        raise Exception(f'Unexpected Characters in {scraped_bool_col_}: {unique_bools}')\n",
    "\n",
    "    # confirm that there's only one entry corresponding to this URL\n",
    "    if checklist[URL_col].dropna().nunique() != len(checklist[URL_col].dropna()):\n",
    "        raise Exception(f'Duplicate URLs in {URL_col}')\n",
    "\n",
    "    # update value\n",
    "    checklist.set_index(URL_col, \n",
    "                        inplace=True)\n",
    "    checklist.loc[URL_of_newly_scraped, scraped_bool_col_] = bool_to_log_\n",
    "\n",
    "    # save/export\n",
    "    checklist.to_csv(checklist_path,\n",
    "                     index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # config for readability\n",
    "    scraped_bool_col = CHECKLIST_COL_NAMES['SCRAPED_BOOL']\n",
    "    url_col = CHECKLIST_COL_NAMES['URL']\n",
    "    id_col = CHECKLIST_COL_NAMES['ID']\n",
    "\n",
    "    # read & clean\n",
    "    checklist = pd.read_csv(CHECKLIST_REL_PATH)\n",
    "    checklist[ scraped_bool_col ] = checklist[scraped_bool_col].map({False:0, True:1}).fillna(checklist[scraped_bool_col])\n",
    "\n",
    "    # filter out \n",
    "    not_scraped_yet_filt = checklist[scraped_bool_col]==0\n",
    "\n",
    "    for i, row in checklist[not_scraped_yet_filt].iterrows():\n",
    "        first_delay = max(np.random.normal(loc=.2, scale=.1, size=1)[0], 0)\n",
    "        time.sleep(first_delay)\n",
    "        print(i, row['Question'])\n",
    "\n",
    "        # config for readability\n",
    "        url_to_scrape = row[url_col]\n",
    "        id = row[id_col]\n",
    "        \n",
    "        bool_to_log = scrape_and_save(URL_to_scrape=url_to_scrape,\n",
    "                                      scraped_data_save_path=SCRAPED_DATA_FOLDER_REL_PATH,\n",
    "                                      scraped_filename=id,\n",
    "                                      ID=id)\n",
    "        \n",
    "        update_checklist_table(checklist_path=CHECKLIST_REL_PATH,\n",
    "                               scraped_bool_col_=scraped_bool_col,\n",
    "                               URL_col=url_col,\n",
    "                               URL_of_newly_scraped=url_to_scrape,\n",
    "                               bool_to_log_=bool_to_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d09275b535ea37cfd4285bf752107cd317aade3824f940e0947870083b34247c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
