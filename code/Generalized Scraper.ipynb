{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find list of items (checklist) we want scraped in the same way (include URLS and IDs (IDs can just be URL)). Finding this checklist is outside of the scope of this file/scraper. \n",
    "2. For each item X we want to scrape:\n",
    "    * Run scraping function on X (and store scrape of X in folder)\n",
    "        * ```scrape_and_save()```\n",
    "    * Update checklist to reflect that X has been scraped (and save updated version of checklist)\n",
    "        * ```update_checklist_table()```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# rel location of checklist of URLs to scrape\n",
    "CHECKLIST_REL_PATH = '../checklist/checklist.csv'\n",
    "\n",
    "# rel location of folder where we store the scraped data\n",
    "SCRAPED_DATA_FOLDER_REL_PATH = '../scraped_data/'\n",
    "\n",
    "# names of relevant columns in our checklist dataframe\n",
    "CHECKLIST_COL_NAMES=dict(\n",
    "    ID=...,\n",
    "    URL=...,\n",
    "    SCRAPED_BOOL=...\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SCRAPING UNIQUE TO THIS PROJECT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import requests\n",
    "# from bs4 import BeautifulSoup\n",
    "\n",
    "# import time\n",
    "\n",
    "def scrape_and_save(URL_to_scrape:str,\n",
    "                    scraped_data_save_path:str,\n",
    "                    scraped_filename:str) -> int: \n",
    "    \"\"\"\n",
    "    This function grabs data from the URL.\n",
    "    MUST RETURN `True` if the scrape didn't work (i.e., if we want to log np.nan under \"scraped_bool\" field in 'checklist')\n",
    "    \"\"\"\n",
    "\n",
    "    # scrape here. leave the rest of this block's code untouched\n",
    "\n",
    "    # figure out what value to log in the checklist (i.e., was this successfully scraped?)\n",
    "    conditions = ...\n",
    "    if conditions:\n",
    "        bool_to_log = 1\n",
    "    else:\n",
    "        bool_to_log = np.nan\n",
    "    return bool_to_log"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# UPDATE CHECKLIST TABLE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def update_checklist_table(checklist_path:str, \n",
    "                        #    checklist:pd.DataFrame,\n",
    "                           scraped_bool_col_:str,\n",
    "                           URL_col:str,\n",
    "                           URL_of_newly_scraped:str,\n",
    "                           bool_to_log_=False\n",
    "                           ):\n",
    "    \n",
    "    # This slows the code down a bit but unless we're scraping 000s of pages this shouldn't be the biggest bottleneck.\n",
    "    # Reading and saving it every time is also more robust to error.\n",
    "    checklist = pd.read_csv(checklist_path) \n",
    "\n",
    "    # confirm that bools are numerical and not True, False\n",
    "    unique_bools = checklist[scraped_bool_col_].unique().tolist()\n",
    "    if not len(set(unique_bools) - set([0, 1, np.nan])) == 0: # only contains 0, 1, or np.nan\n",
    "        unexpected_chars = ', '.join(list(set(unique_bools) - set([0, 1, np.nan])))\n",
    "        raise Exception(f'Unexpected Characters in {scraped_bool_col_}: {unexpected_chars}')\n",
    "\n",
    "    # confirm that there's only one entry corresponding to this URL\n",
    "    if checklist[URL_col].nunique() != len(checklist):\n",
    "        raise Exception(f'Duplicate URLs in {URL_col}')\n",
    "\n",
    "    # update value\n",
    "    checklist.set_index(URL_col, \n",
    "                        inplace=True)\n",
    "    checklist.loc[URL_of_newly_scraped, scraped_bool_col_] = bool_to_log_\n",
    "\n",
    "    # save/export\n",
    "    checklist.to_csv(checklist_path,\n",
    "                     index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MAIN FNC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    # config for readability\n",
    "    scraped_bool_col = CHECKLIST_COL_NAMES['SCRAPED_BOOL']\n",
    "    url_col = CHECKLIST_COL_NAMES['URL']\n",
    "    id_col = CHECKLIST_COL_NAMES['ID']\n",
    "\n",
    "    # read & clean\n",
    "    checklist = pd.read_csv(CHECKLIST_REL_PATH)\n",
    "    checklist[ scraped_bool_col ] = checklist[scraped_bool_col].map({False:0, True:1}).fillna(checklist[scraped_bool_col])\n",
    "\n",
    "    # filter out \n",
    "    not_scraped_yet_filt = checklist[scraped_bool_col]==0\n",
    "\n",
    "    for i, row in checklist[not_scraped_yet_filt].iterrows():\n",
    "        # config for readability\n",
    "        url_to_scrape = row[url_col]\n",
    "        id = row[id_col]\n",
    "        \n",
    "        bool_to_log = scrape_and_save(URL_to_scrape=url_to_scrape,\n",
    "                                      scraped_data_save_path=SCRAPED_DATA_FOLDER_REL_PATH,\n",
    "                                      scraped_filename=id)\n",
    "        \n",
    "        update_checklist_table(checklist_path=CHECKLIST_REL_PATH,\n",
    "                               scraped_bool_col_=scraped_bool_col,\n",
    "                               URL_col=url_col,\n",
    "                               URL_of_newly_scraped=url_to_scrape,\n",
    "                               bool_to_log_=bool_to_log)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d09275b535ea37cfd4285bf752107cd317aade3824f940e0947870083b34247c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
