{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# OUTLINE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Find list of items (checklist) we want scraped in the same way (include URLS and IDs (IDs can just be URL)). Finding this checklist is outside of the scope of this file/scraper. \n",
    "2. For each item X we want to scrape:\n",
    "    * Run scraping function on X (and store scrape of X in folder)\n",
    "        * scraper.py\n",
    "    * Update checklist to reflect that X has been scraped (and save updated version of checklist)\n",
    "        * update_checklist.py\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CONFIG"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "CHECKLIST_REL_PATH = ...\n",
    "SCRAPED_DATA_FOLDER_REL_PATH = ...\n",
    "\n",
    "CHECKLIST_COL_NAMES=dict(\n",
    "    ID=...,\n",
    "    URL=...,\n",
    "    SCRAPED_BOOL=...,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT PACKAGES"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# IMPORT FUNCTION(S)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scraper_fncs.scrape_and_update_checklist import scrape_and_update_checklist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RUN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main():\n",
    "    checklist = pd.read_csv(CHECKLIST_REL_PATH)\n",
    "    for url_to_scrape in checklist.query(f'{CHECKLIST_COL_NAMES[\"SCRAPED_BOOL\"]}==1')[CHECKLIST_COL_NAMES['URL']]:\n",
    "        scrape_and_update_checklist(checklist=checklist,\n",
    "                                    scraped_bool_col=CHECKLIST_COL_NAMES['SCRAPED_BOOL'],\n",
    "                                    URL_col=CHECKLIST_COL_NAMES['ID'],\n",
    "                                    URL_to_scrape=url_to_scrape,\n",
    "                                    scraped_data_save_path=SCRAPED_DATA_FOLDER_REL_PATH)\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    main()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "d09275b535ea37cfd4285bf752107cd317aade3824f940e0947870083b34247c"
  },
  "kernelspec": {
   "display_name": "Python 3.9.7 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
